---
title: "Data Science Capstone Milestone Report"
author: "samaa essa"
date: "12/23/2020"
output: html_document
---
## Overview

It is the Milestone Report for the Coursera Data Science Capstone project. In this capstone, we will be applying data science in the area of natural language processing. The project is sponsored by SwiftKey.

The final objective of the project is to create text-prediction application with R Shiny package that predicts words using a natural language processing model i.e. creating an application based on a predictive model for text. Given a word or phrase as input, the application will try to predict the next word. The predictive model will be trained using a corpus, a collection of written texts, called the HC Corpora which has been filtered by language.

But, this milestone report describes the exploratory data analysis of the Capstone Dataset.

## Task Gaol

The goal of this project is just to display that youâ€™ve gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to:

In short this markup document shows:

1. Demonstration in loading and cleaning the data.
2. Summary statistics about the data sets.
3. First interesting findings.
4. Feedback for creating a prediction algorithm and Shiny app.

## About Data

The corpora are collected from publicly available sources by a web crawler. The crawler checks for language, so as to mainly get texts consisting of the desired language*.

Each entry is tagged with it's date of publication. Where user comments are included they will be tagged with the date of the main entry.

Each entry is tagged with the type of entry, based on the type of website it is collected from (e.g. newspaper or personal blog) If possible, each entry is tagged with one or more subjects based on the title or keywords of the entry (e.g. if the entry comes from the sports section of a newspaper it will be tagged with "sports" subject).In many cases it's not feasible to tag the entries (for example, it's not really practical to tag each individual Twitter entry, though I've got some ideas which might be implemented in the future) or no subject is found by the automated process, in which case the entry is tagged with a '0'.

To save space, the subject and type is given as a numerical code.

Once the raw corpus has been collected, it is parsed further, to remove duplicate entries and split into individual lines. Approximately 50% of each entry is then deleted. Since you cannot fully recreate any entries, the entries are anonymised and this is a non-profit venture I believe that it would fall under [Fair Use](https://web-beta.archive.org/web/20160930083655/http://en.wikipedia.org/wiki/Fair_use).

* You may still find lines of entirely different languages in the corpus. There are 2 main reasons for that:1. Similar languages. Some languages are very similar, and the automatic language checker could therefore erroneously accept the foreign language text.2. "Embedded" foreign languages. While a text may be mainly in the desired language there may be parts of it in another language. Since the text is then split up into individual lines, it is possible to see entire lines written in a foreign language.Whereas number 1 is just an out-and-out error, I think number 2 is actually desirable, as it will give a picture of when foreign language is used within the main language.

Content archived from heliohost.org on September 30, 2016 and retrieved via Wayback Machine on April 24, 2017. https://web-beta.archive.org/web/20160930083655/http://www.corpora.heliohost.org/aboutcorpus.html 

Note : At this Task I use only English Text

### Load Library
```{r include=FALSE}
library(dplyr)
library(doParallel)
library(stringi)
library(SnowballC)
library(tm)
library(RWeka)
library(ggplot2)
```

### Import Data

```{r}
con <- file("en_US/en_US.news.txt", open="r")
news_text <- readLines(con); close(con)

con <- file("en_US/en_US.blogs.txt", open="r")
blogs_text <- readLines(con); close(con) 

con <- file("en_US/en_US.twitter.txt", open="r")
twit_text <- readLines(con); close(con)
rm(con)
```

## Files Statistics Summaries

```{r}
stri_stats_general(news_text)
stri_stats_general(blogs_text)
stri_stats_general(twit_text)

```


## Clean Data

```{r}
# Sampling
set.seed(2510)
blogs_text <- sample(blogs_text, size = 1000)
news_text <- sample(news_text, size = 1000)
twit_text <- sample(twit_text, size = 1000)

# Union corpora
corpora <- c(news_text,blogs_text,twit_text)
corpora <- VectorSource(corpora)
corpora <- VCorpus(corpora)

# Text Preprocessing 
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpora <- tm_map(corpora, toSpace, "/|@|//|$|:|:)|*|&|!|?|_|-|#|")
corpora <- tm_map(corpora, removeNumbers)
corpora <- tm_map(corpora, content_transformer(tolower))
corpora <- tm_map(corpora, removeWords, stopwords("english"))
corpora <- tm_map(corpora, removePunctuation)
corpora <- tm_map(corpora, stemDocument)
corpora <- tm_map(corpora, stripWhitespace)
```

## Document Term Matrices 

create 1,2,3 grams
```{r}
unigrams <- TermDocumentMatrix(corpora)
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigrams <- TermDocumentMatrix(corpora, control = list(tokenize = bigram))
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigrams <- TermDocumentMatrix(corpora, control = list(tokenize = trigram))
```

## Data Exploration

Plot Unigram Frequencies
```{r}
unigrams_freq <- rowSums(as.matrix(unigrams))
unigrams_freq <- sort(unigrams_freq, decreasing = TRUE)
unigrams_freq_df <- data.frame(word = names(unigrams_freq), freq=unigrams_freq)
ggplot(unigrams_freq_df[1:50, ], aes(reorder(word,-freq), freq)) +
    geom_bar(stat = "identity") +
    ggtitle("Top 100 Unigrams") +
    xlab("Unigrams") + ylab("Frequency") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    theme(plot.margin = unit(c(1,1,1,1), "cm"))
```

Plot Bigram Frequencies
```{r}
bigrams_freq <- rowSums(as.matrix(bigrams))
bigrams_freq <- sort(bigrams_freq, decreasing = TRUE)
bigrams_freq_df <- data.frame(word = names(bigrams_freq), freq=bigrams_freq)
ggplot(bigrams_freq_df[1:50, ], aes(reorder(word,-freq), freq)) +
    geom_bar(stat = "identity") +
    ggtitle("Top 100 Bigrams") +
    xlab("Bigrams") + ylab("Frequency") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    theme(plot.margin = unit(c(1,1,1,1), "cm"))
```

Plot Trigram Frequencies
```{r}
trigrams_freq <- rowSums(as.matrix(trigrams))
trigrams_freq <- sort(trigrams_freq, decreasing = TRUE)
trigrams_freq_df <- data.frame(word = names(trigrams_freq), freq=trigrams_freq)
ggplot(trigrams_freq_df[1:50, ], aes(reorder(word,-freq), freq)) +
    geom_bar(stat = "identity") +
    ggtitle("Top 100 Trigrams") +
    xlab("Trigrams") + ylab("Frequency") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    theme(plot.margin = unit(c(1,1,1,1), "cm"))
```


